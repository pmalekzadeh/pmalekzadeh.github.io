<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Nuvo: Neural UV Mapping">
    <meta name="keywords" content="NeRF">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under
         partially
    observability</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"
        integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/video_comparison.js"></script>
    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üèÅ</text></svg>">
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">

                        <h1 class="title is-1 publication-title">Active Inference and Reinforcement Learning: A unified inference on 
                            continuous state and action spaces under partially
                        observability</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://pmalekzadeh.github.io">Parvin Malekzadeh</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://www.dsp.utoronto.ca/~kostas/">Konstantinos N.
                                    Plataniotis</a><sup>1</sup>,</span>                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>University of Toronto,</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2212.07946.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>

                                <!-- <span class="link-block">
                                    <a href="https://github.com/pmalekzadeh/A-robust-quantile-huber-loss" 
                                     class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            -->
               

                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <hr />

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
                <p>
                    Reinforcement learning (RL) has garnered significant attention for developing  
                    agents that aim to maximize
                    rewards, specified by an external supervisor, within
                    fully observable environments. However, many real-world problems involve partial or
                    noisy observations, where agents do not have access to complete and accurate 
                    information about the environment. These
                    problems are commonly formulated as partially
                    observable Markov decision processes (POMDPs). Previous studies have tackled RL in
                    POMDPs by either incorporating the memory of past actions and observations or by 
                    inferring the true state of the
                    environment from observed data. Nevertheless, aggregating
                    observations and actions over time becomes impractical in high-dimensional continuous spaces. 
                    Furthermore,
                    inference-based RL approaches often require a large number
                    of environmental samples to perform well, as they focus solely on reward maximization and 
                    neglect uncertainty in the
                    inferred state. </p>
                <p>
                    Active inference (AIF) is a framework naturally formulated in POMDPs and directs 
                    agents to select actions by minimizing a
                    function called expected free energy (EFE). This supplies reward-maximizing 
                    (or exploitative) behaviour, as in RL, with
                    information-seeking (or exploratory) behaviour.
                    Despite this exploratory behaviour of AIF, its usage is limited to small and 
                    discrete spaces due
                    to the computational challenges associated with EFE.
                    In this paper, we propose a unified principle that establishes a theoretical connection
                    between AIF and RL, enabling seamless integration of these two approaches and 
                    overcoming their aforementioned
                    limitations in continuous space POMDP settings. 
                 </p>
                <p> We
                substantiate our findings with rigorous theoretical analysis, providing novel perspectives
                for utilizing AIF in the
                design and implementation of artificial agents. Experimental results demonstrate
                the superior learning capabilities of
                our method compared
                to other alternative RL approaches in solving partially observable tasks with continuous
                spaces. Notably, our approach harnesses information-seeking exploration, enabling it
                to effectively solve reward-free problems.
            </p>
                </p>
            </div>
        </div>

        </div>
    </section>
    <hr />

    <!-- Video
    <section class="section">
        <div class="container is-max-desktop">

            <h2 class="title is-3">Video</h2>
            <div class="container is-max-desktop">

                <div class="publication-video">
                    <iframe src="https://www.youtube.com/embed/hmJiOSTDQZI?si=Xq4TWH4D9BwMmlp7" frameborder="0"
                        allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </div>
            </div>
        </div>
        </div>
    </section> -->
    

    

    <section class="section">
        <div class="container is-max-desktop">

            <h2 class="title is-3">Comparison with other RL methods</h2>

            <div class="content has-text-justified">
                <p>
                    Table 1 compares human-normalized
                    scores resulted from our loss function (GL) and its apprxoimation (GLA) with RL literature, 
                    which used the quantile Huber
                    loss with a fixed k=1, neglecting the impact of varying k on performance.

                    <figure>
                        <img src="static/images/Table.JPG" alt="MY ALT TEXT" style="max-width: 70%;">
                                                    <figcaption style=" text-align: center;"></figcaption>
                    </figure>

                    To test our interpretation for k in a more realistic
                    setting, we assess its performance in option hedging,
                    a risk-aware financial application where the objective is to optimize the 
                    Conditional Value-at-Risk at
                    a 95% confidence level (CVaR95) for total rewards. 
                    Interestingly, the optimal value of
                    k for D4PG is not 1, and D4PG-GLA‚Äôs estimated
                    b-value during training aligns with this optimal k
                    (‚âà2). These results emphasize the advantages of finetuning k in the quantile Huber loss and demonstrate
                    the effectiveness of our k interpretation, reducing
                    the need for extensive parameter searches.

                    <figure>
                        <img src="static/images/Hedge.JPG" alt="MY ALT TEXT" style="max-width: 70%;">
                                                                        <figcaption style=" text-align: center;"></figcaption>
                    </figure>

                
            </div>

        </div>
    </section>

    <!-- Paper poster -->
    <!-- <section class="hero is-small is-light">
        <div class="hero-body">
            <div class="container">
                <h2 class="title">Poster</h2>
    
                <iframe src="static/img/DataDAM-ICCV23-Poster.pdf" width="100%" height="550">
                </iframe>
    
            </div>
        </div>
    </section> -->
    <!--End paper poster -->

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>
            @article{malekzadeh2024robust,
            title={A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning},
            author={Malekzadeh, Parvin and Plataniotis, Konstantinos N and Poulos, Zissis and Wang, Zeyu},
            journal={arXiv preprint arXiv:2401.02325},
            year={2024}
            }
        </code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website design comes from <a href="https://keunhong.com/">Keunhong Park's</a> <a
                            href="https://camp-nerf.github.io/">CamP project page</a>.
                    </p>
                </div>
            </div>
        </div>
        </div>
    </footer>

</body>

</html>