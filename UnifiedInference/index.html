<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Nuvo: Neural UV Mapping">
    <meta name="keywords" content="NeRF">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under
         partially
    observability</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"
        integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/video_comparison.js"></script>
    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üèÅ</text></svg>">
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">

                        <h1 class="title is-1 publication-title">Active Inference and Reinforcement Learning: A unified inference on 
                            continuous state and action spaces under partial
                        observability</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://pmalekzadeh.github.io">Parvin Malekzadeh</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://www.dsp.utoronto.ca/~kostas/">Konstantinos N.
                                    Plataniotis</a><sup>1</sup></span>                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>University of Toronto</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2212.07946.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>

                                <!-- <span class="link-block">
                                    <a href="https://github.com/pmalekzadeh/A-robust-quantile-huber-loss" 
                                     class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            -->
               

                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <hr />
    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <figure poster="" id="tree" autoplay controls muted loop height="70%">
                    <!-- Your video here -->
                    <img src="static/images/Block.JPG" alt="MY ALT TEXT" />
                </figure>
                <h2 class="subtitle has-text-centered">
                    AIF, RL, and unified inference frameworks. Blue, red, and green rectangles represent aspects of AIF, RL, and unified
                    inference, respectively. The square dot rectangle
                    indicates the decision spaces under which each framework is formulated.
                </h2>
            </div>
        </div>
    </section>
    
    <hr />

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
                <p>
                    Reinforcement learning (RL) has garnered significant attention for developing  
                    agents that aim to maximize
                    rewards, specified by an external supervisor, within
                    fully observable environments. However, many real-world problems involve partial or
                    noisy observations, where agents do not have access to complete and accurate 
                    information about the environment. These
                    problems are commonly formulated as partially
                    observable Markov decision processes (POMDPs). Previous studies have tackled RL in
                    POMDPs by either incorporating the memory of past actions and observations or by 
                    inferring the true state of the
                    environment from observed data. Nevertheless, aggregating
                    observations and actions over time becomes impractical in high-dimensional continuous spaces. 
                    Furthermore,
                    inference-based RL approaches often require a large number
                    of environmental samples to perform well, as they focus solely on reward maximization and 
                    neglect uncertainty in the
                    inferred state. </p>
                <p>
                    Active inference (AIF) is a framework naturally formulated in POMDPs and directs 
                    agents to select actions by minimizing a
                    function called expected free energy (EFE). This supplies reward-maximizing 
                    (or exploitative) behaviour, as in RL, with
                    information-seeking (or exploratory) behaviour.
                    Despite this exploratory behaviour of AIF, its usage is limited to small and 
                    discrete spaces due
                    to the computational challenges associated with EFE.
                    In this paper, we propose a unified principle that establishes a theoretical connection
                    between AIF and RL, enabling seamless integration of these two approaches and 
                    overcoming their aforementioned
                    limitations in continuous space POMDP settings. 
                 </p>
                <p> Experimental results demonstrate
                the superior learning capabilities of
                our method compared
                to other alternative RL approaches in solving partially observable tasks with continuous
                spaces. Notably, our approach harnesses information-seeking exploration, enabling it
                to effectively solve reward-free problems.
            </p>
                </p>
            </div>
        </div>

        </div>
    </section>
    <hr />

    <!-- Video
    <section class="section">
        <div class="container is-max-desktop">

            <h2 class="title is-3">Video</h2>
            <div class="container is-max-desktop">

                <div class="publication-video">
                    <iframe src="https://www.youtube.com/embed/hmJiOSTDQZI?si=Xq4TWH4D9BwMmlp7" frameborder="0"
                        allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </div>
            </div>
        </div>
        </div>
    </section> -->
    

    

    <section class="section">
        <div class="container is-max-desktop">

            <h2 class="title is-3">Comparison</h2>

            <div class="content has-text-justified">
                <p>
                    Table 1 compares foundational elements and decision-making strategies within
                    RL, AIF, and our proposed unified inference approach, with a particular focus on their
                    application in continuous decision spaces.
                
                <figure>
                    <img src="static/images/Table1.JPG" alt="MY ALT TEXT" style="max-width: 80%;">
                    <figcaption style=" text-align: center;"></figcaption>
                </figure>
                </p>

            </div>

        </div>
    </section>

    <hr />

    <section class="section">
        <div class="container is-max-desktop">
    
            <h2 class="title is-3">The Proposed Unified Iteration Framework</h2>
    
            <div class="content has-text-justified">
                <p>
    
                <figure>
                    <img src="static/images/Summary.JPG" alt="MY ALT TEXT" style="max-width: 80%;">
                    <figcaption style=" text-align: center;"></figcaption>
                </figure>
                </p>
    
            </div>
    
        </div>
    </section>

        <hr />

    <section class="section">
        <div class="container is-max-desktop">
    
            <h2 class="title is-3">Experimental Results</h2>
    
            <!-- Sub-section 1: Effectiveness and Performance -->
            <div class="content has-text-justified">
                <h3 class="title is-4">Evaluation on partial and noisy observation tasks</h3>
                <p>
                    The results presented in Table 2 highlight the effectiveness of the proposed
                    unified inference algorithm in various aspects: (i) It successfully generalizes MDP
                    actor-critic methods to the POMDP setting, allowing for more effective exploration
                    and learning under partial observability. (ii) It outperforms memory-based approaches
                    in scenarios with noisy observations, indicating the advantage of leveraging the belief
                    state representation in handling observation noise. (iii) The inclusion of the information
                    gain intrinsic term into the generalized actor-critic methods improves their robustness
                    to noisy observations.
                </p>
                <figure>
                    <img src="static/images/Results1.JPG" alt="Graph showing experimental results" style="max-width: 100%;">
                </figure>
                </div>

            <!-- Sub-section 2: Comparative Analysis -->
            <div class="content has-text-justified">
                <h3 class="title is-4">Evaluation of exploratory abilities</h3>
                <p>
                    Fig. 2. compares the exploratory behaviors of our G-SAC agent with ICAM, RND, and VIME, which incorporate
                    information-oriented exploratory terms as intrinsic rewards in SAC. ICM and RND incorporate 
                    the prediction error of a transition model, and VIME employs intrinsic rewards that are designed to maximize the information
                    gain concerning the parameters of the Bayesian neural network.
                    <br />
                    The
                    results indicate that methods with the intrinsic reward being information gain, namely
                    G-SAC and VIME, learn much faster, suggesting that their exploration is more effective
                    than the baseline agent‚Äôs exploration. On the other hand, the performance of ICM
                    and RND is strongly undermined by randomness. It is widely recognized that intrinsic motivation based on the prediction error
                    of a transition model is sensitive to the inherent stochasticity of the environment (Burda
                    et al., 2018).
            
            </p>
            <figure>
                <img src="static/images/Results2.JPG" alt="Graph comparing exploration behaviors" style="max-width: 100%;">
            </figure>
            </div>
            </div>
            </section>

    <!-- Paper poster -->
    <!-- <section class="hero is-small is-light">
        <div class="hero-body">
            <div class="container">
                <h2 class="title">Poster</h2>
    
                <iframe src="static/img/DataDAM-ICCV23-Poster.pdf" width="100%" height="550">
                </iframe>
    
            </div>
        </div>
    </section> -->
    <!--End paper poster -->

    <hr />
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>
            @article{malekzadeh2024active,
            title={Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under
            partial observability},
            author={Parvin Malekzadeh, Konstantinos N. Plataniotis},
            journal={arXiv:2212.07946v2},
            year={2024}
            }
        </code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website design comes from <a href="https://keunhong.com/">Keunhong Park's</a> <a
                            href="https://camp-nerf.github.io/">CamP project page</a>.
                    </p>
                </div>
            </div>
        </div>
        </div>
    </footer>

</body>

</html>